<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://collinschwantes.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://collinschwantes.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-02T16:35:22+00:00</updated><id>https://collinschwantes.github.io/feed.xml</id><title type="html">blank</title><subtitle>a personal website for Collin Schwantes </subtitle><entry><title type="html">Make your open data more useable</title><link href="https://collinschwantes.github.io/blog/2025/data-quick-wins/" rel="alternate" type="text/html" title="Make your open data more useable"/><published>2025-12-02T01:39:00+00:00</published><updated>2025-12-02T01:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2025/data-quick-wins</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2025/data-quick-wins/"><![CDATA[<p>Making open data more useful for yourself and others may be easier than you think. Here are some quick wins you can employ with every open dataset you publish.</p> <ol> <li>Include metadata for your data. This may be a schema or a data dictionary. Whatever form it takes, providing definitions for the fields in your data will make them immensely more useful. It is obvious to you, the person who has been working on this project the most, what each field means, but not to the rest of us. Metadata will also give you an opportunity to think critically about what each field means in your data and how best to describe it. Most importantly, metadata will give future you a better understanding of what these data mean when you come back to them in 6 months or a year. If you can, use a controlled vocabulary to define terms in your data.</li> <li>Include project metadata. Tell people who created the data, when, and how. Provide links to publications and protocols if applicable. This makes it much easier to contextualize the data and to find them via search.</li> <li>Put your data in a place people are likely to find it. If no discipline specific place exists, put it in a generalist repository like Zenodo, OSF, or Figshare. Github repositories are a good start but should not be a final location.</li> <li>Publish your data in a non-proprietary file format. If you’re creating tabular data, CSV’s are a great way to store your data. If your data are too big for a CSV, consider <a href="https://parquet.apache.org/">parquet</a>.</li> <li>It may be 2025, but keep punctuation (except underscores) and emojis out of column headers/field names. Why? Because punctuation and emojis make basic tools like regular expressions harder to use. Many programming languages interpret things like <code class="language-plaintext highlighter-rouge">!</code>, <code class="language-plaintext highlighter-rouge">.</code>, <code class="language-plaintext highlighter-rouge">$</code>, or <code class="language-plaintext highlighter-rouge">?</code> in a particular way and having to code around them creates friction. Even seemingly simple characters like the humble dash can be tricky (<a href="https://www.compart.com/en/unicode/category/Pd">unicode dash encodings</a>). Emojis may be unicode but they are often compound unicode characters <code class="language-plaintext highlighter-rouge">person + dance + skin-color-2</code>, which can cause inscrutable encoding errors or unexpected issues.</li> <li>Use consistent styling in your column headers. <code class="language-plaintext highlighter-rouge">snake_case</code> and <code class="language-plaintext highlighter-rouge">camelCase</code> are nice because they are human and machine readable. Having inconsistent styling means users are more likely to commit typographical errors.</li> <li>If applicable, use persistent digital identifiers like DOIs, <a href="https://orcid.org/">ORCIDs</a>, <a href="https://ror.org/">ROR ids</a> or permanent URLS when referencing outside resources. If you have a database of journal articles, include the DOIs to make referencing the articles faster. A lot of metadata about a journal article can be extracted if you have the DOI handy.</li> </ol>]]></content><author><name></name></author><category term="data"/><category term="publishing"/><category term="fair"/><summary type="html"><![CDATA[Quick wins when publishing open data]]></summary></entry><entry><title type="html">Course on Cloud Native GIS</title><link href="https://collinschwantes.github.io/blog/2025/cloud-native-gis/" rel="alternate" type="text/html" title="Course on Cloud Native GIS"/><published>2025-08-30T17:39:00+00:00</published><updated>2025-08-30T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2025/cloud-native-gis</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2025/cloud-native-gis/"><![CDATA[<p>This course was produced and run by a group called thriveGEO for Yale University - it focused on the fundamentals of cloud-computing and specialized datastores/file formats for spatio-temporal data.</p> <p>One major challenge for any researcher is data discovery. The <a href="https://stacspec.org/en">STAC specification</a> provides a common and extensible way to describe geospatial information, improving discoverability. A STAC contains multiple levels of metadata that allow you to find a specific data product and query that product so that you receive just the data you need.</p> <p>This ability to query is critical for performant cloud-native geospatial analysis because data latency is the workflow killer. Moving the appropriate amount of data (potentially in parallel) instead of downloading a whole the dataset saves time and resources.</p> <p>In order for this all to work effectively, you need file formats like <a href="https://cogeo.org/">COG</a> and <a href="https://geoparquet.org/">geoparquet</a> that allow users to slice the data into small parts and libraries like DASK that improve parallelization. With appropriately sized chunks and well defined, parallel tasks, a research can to analyses over large swaths of space and time efficiently.</p> <p>Finally, the course talked about different analysis platforms that range from custom built cloud environments to plug-and-play systems. All of the plug-and-play platforms that were presented immediately raised concerns about vendor lock and reproducibility.</p> <p>The cloud-native geospatial space is still a developing area and will likely progress significantly in the coming years. The STAC ecosystem could benefit from better search funcitonality by surfacing lower level metadata for each catalog. From a FAIR research perspective, some aspects cloud native are wonderful - e.g. coding queries to access data and running analysis on “not your machine” - while others - lack of versioning and potential for vendors to dispappear - give me pause.</p>]]></content><author><name></name></author><category term="cloud-computing"/><category term="python"/><category term="COG"/><category term="geoparquet"/><category term="latency"/><summary type="html"><![CDATA[Notes on cloud native gis]]></summary></entry><entry><title type="html">Rix with Bruno Rodrigues</title><link href="https://collinschwantes.github.io/blog/2025/rix/" rel="alternate" type="text/html" title="Rix with Bruno Rodrigues"/><published>2025-03-13T17:39:00+00:00</published><updated>2025-03-13T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2025/rix</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2025/rix/"><![CDATA[<p>https://github.com/b-rodrigues/r-medicine-rix</p> <p><strong>Definition of reproducibility</strong> - recover exactly the same resutls from an analysis (replicate)</p> <p>1) How easy for someone else to use your code?<br/> 2) How easy to update the project? (data/package/etc)<br/> 3) How easy to reuse the code yourself?<br/> 4) How stable is the output through time?</p> <p>Reproducibility exists on a spectrum.</p> <p>The R version used, the package version, OS, and hardware can all potentially impact your ability to reproduce an analysis.</p> <p>Reproducibility is not trivial.</p> <p>Reproducibility tools:</p> <ul> <li><code class="language-plaintext highlighter-rouge">renv</code> - package management and env documentation - does not allow you to restore R version, old package installs often fail :( or require a lot of effort</li> <li><code class="language-plaintext highlighter-rouge">docker</code> - capture system level libraries/compute env - build images and run containers - has all software and code needed for project. Combined with <code class="language-plaintext highlighter-rouge">renv</code> you are happy :) <ul> <li>useful and widely used</li> <li>checkour rocker project</li> <li>steep learning curve</li> <li>basically setting up a linux os</li> <li>can create a single point of failure</li> <li>not designed for reproducibility</li> </ul> </li> </ul> <h2 id="what-is-nix">What is Nix?</h2> <p>Package manager - any piece of software (firefox, R, r packages, python packages)</p> <ul> <li>120k pieces of software</li> <li>Extremely fresh</li> </ul> <p>Nix facilitates gold standard reproducibility because its designed with reproducibility in mind. Nix deals with everything by writing a “nix expression” written in nix. Nix always builds packages in the exact same way. Rix provides an friendly R based interface for Nix.</p> <p>Can start your nix file from renv.lock file! You can use targets in nix.</p> <p>You can add an IDE and work interactively in your preferred IDE.</p> <p>nix expressions can be used with CI and even in containers.</p> <p>can add any <code class="language-plaintext highlighter-rouge">system_pkgs</code> found here: https://search.nixos.org/packages</p> <p>Nix uses WSL on windows. If you can’t have WSL, you cant use windows.</p> <p>Any pain points when collaborating?</p> <ul> <li>if you’re using nix, use nix. Local install of R/r packages can be disrupted by nix.</li> <li>some issues with mac - certain system tools may not be available on macos (e.g. chromium dependencies)</li> </ul> <p>Last Mile - how do we make this more accessible for field focused collegues?</p> <ul> <li>Need to have someone on the team that is very close to field focused people and very technically savvy e.g. research software engineer</li> </ul> <p>My thoughts:</p> <ul> <li>Looking forward to playing around with this!</li> <li>Some of the requirements make it impracticle for certain types of collaborators.</li> <li>It might be a good idea to use <code class="language-plaintext highlighter-rouge">targets</code> and <code class="language-plaintext highlighter-rouge">renv</code> throughout development and wrap everything up in nix and docker at specific release points.</li> </ul>]]></content><author><name></name></author><category term="reproducibility"/><category term="R"/><summary type="html"><![CDATA[Notes on R package Rix]]></summary></entry><entry><title type="html">Writing a python package with Hatch and pyOpenSci</title><link href="https://collinschwantes.github.io/blog/2025/pypackage/" rel="alternate" type="text/html" title="Writing a python package with Hatch and pyOpenSci"/><published>2025-03-13T17:39:00+00:00</published><updated>2025-03-13T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2025/pypackage</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2025/pypackage/"><![CDATA[<p>I recently published a new R package called <a href="https://github.com/viralemergence/virionData"><code class="language-plaintext highlighter-rouge">virionData</code></a>. It is essentially a wrapper for the Zenodo api and is designed to 1) provide access to the virion data stored on zenodo and 2) make it easier to track the version of the virion data one is using. After announcing the publication of the R package, a colleague almost immediately asked for a python version. Fair, why should they have to install R a dependency in their pipeline just to get the data?</p> <p>So now I am setting out to write a python package using Hatch and guidance from <a href="https://www.pyopensci.org/python-package-guide/tutorials/get-to-know-hatch.html#">pyOpenSci</a>.</p> <p>First impressions:</p> <p>In R, we can use <code class="language-plaintext highlighter-rouge">devtools</code>, <code class="language-plaintext highlighter-rouge">testthat</code>, and <code class="language-plaintext highlighter-rouge">usethis</code> to develop a fairly robust package. Granted those three packages load a host of others, the python package development ecosystem seems much less consolidated.</p> <p>I almost immediately ran into an issue with hatch. I wanted to call my package pyVirionData and that meant that my top level directory and the package directory in source had the same name. This caused some regex functions in hatch to fail when attempting to build the package. I may have also run an unnecessary <code class="language-plaintext highlighter-rouge">hatch init</code> call. So I started over and changed the name of the package to py-virion-data, the sub directory became py_virion_data and everything works as expected.</p> <p>As someone who primarialy programs in R, I’m a little nervous about publishing python code. I have developed a strong muscle memory for functional programming and will likley write code in that flavor (as opposed to more object oriented paradigms).</p> <p>After the initial hiccup with the package name, the tutorials seems to be running smoothly.</p> <h3 id="writing-python-code---youre-about-enter-a-stream-of-conciousness-section">Writing python code - you’re about enter a stream of conciousness section</h3> <p>Using vscode the docString extension is really helpful.</p> <p>Having to think in terms of classes makes me more economical.</p> <p>Oh yeah, you have to import functions between files, its not all just global like in R.</p> <p>python does a good job with json out of the box. this is nice.</p> <p>File paths are a mess. thank the gods for r’s FS package.</p> <p>The pythonFileSystem docs are confusing. There are functions for fs.path.BLAH and there are functions for different types of file systems.</p> <p>All file systems have api calls listed here: https://pyfilesystem2.readthedocs.io/en/latest/interface.html</p> <p>these are called with something like</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from fs.osfs import OSFS
 
fs_home = OSFS.home(".")
fs_home.makedir("hello") # makes a directory relative to working directory

</code></pre></div></div> <p>fs itself has classes and methods.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import fs
helloworld_path = fs.paths.join("hello","world")
</code></pre></div></div> <p>we can use them together with something like</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import fs
from fs.osfs import OSFS
fs_home = OSFS.home(".")
fs_home.makedir("hello")

helloworld_path = fs.paths.join("hello","world")
# note that makedir doesnt make directories recursively.
# there are functions in fs to build up components of a path
# and I am sure they would be useful for creating recursive
# directories
fs_home.makedir(helloworld_path)
</code></pre></div></div> <p>For loops on items are awesome.</p> <h3 id="merge-pull-requests">merge pull requests</h3> <p>I have gotten into the habit of splitting R functions into their own files. When adjudicating a PR, I check that each file/function meets my criteria for documentation and clarity. Each correction I make is reflected in only that file so I can use the “viewed” feature in github to great effect. With a monolith file that defines a class and its methods, I have to carefully check each section. While polishing up documentation and refining methods, the monolith file is changed, so I have to be extra vigilent about where I made changes. This is probably a function of not using subclasses/inheritance properly - though as of this writing the class definition is only ~350 lines.</p> <h3 id="publishing-workflow">publishing workflow</h3> <p>pypi using hatch</p> <p>1) <code class="language-plaintext highlighter-rouge">hatch version</code> 2) <code class="language-plaintext highlighter-rouge">hatch build</code> 3) <code class="language-plaintext highlighter-rouge">hatch publish -r # main or test </code></p> <p>grayskull for conda forge - to do</p> <p>read the docs - to do - this is a little less straightfoward than using <code class="language-plaintext highlighter-rouge">pkgdown</code> in R to build nice docs for your package.</p> <h2 id="conclusion">Conclusion</h2> <p>In general the system for publishing a python package is really straight foward. The PyOpenSci tutorial made the process very smooth. I think that since I had already written the R package and knew what functionality I wanted, it was easier to focus on building the python library. I would definitely recommend this order of operations for anyone coming from R to python because it allows you to focus on the python syntax and structure. Writing out the classes in python made me realize there is a lot of bloat in my R package and gave me ideas about how I could refactor to be more efficient.</p>]]></content><author><name></name></author><category term="reproducibility"/><category term="python"/><summary type="html"><![CDATA[Notes on writing a python package]]></summary></entry><entry><title type="html">Deposits in the Wild</title><link href="https://collinschwantes.github.io/blog/2024/deposits-in-the-wild/" rel="alternate" type="text/html" title="Deposits in the Wild"/><published>2024-08-30T17:39:00+00:00</published><updated>2024-08-30T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2024/deposits-in-the-wild</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2024/deposits-in-the-wild/"><![CDATA[<p>This blog post has been published on the <a href="https://ropensci.org/blog/2024/09/17/deposits-in-the-wild/">ROpenSci blog</a></p> <p>For the better part of a year, I have been looking for an opportunity to use the ROpenSci package <a href="https://docs.ropensci.org/deposits/">{deposits}</a> in my role as the Data Librarian at EcoHealth Alliance. I had done some initial testing with the Mark Padgham, the brilliant person who developed this package, but there weren’t any projects ready for me to put {deposits} through its paces. Enter the <em>Rift Valley Fever Virus in South Africa</em> project, a ten year, multiple part study of humans, wildlife (mosquitoes and wild ungulates), and domestic animals that uses every data store from dropbox to Google drive to Airtable to ODK with a healthy mix file formats for tabular data. Additionally, the PIs on the project are very enthusiastic about making the data FAIR.</p> <p>The team and I put together workflow in <a href="https://books.ropensci.org/targets/">{targets}</a> with the mechanics of ETL largely handled by our <a href="https://ecohealthalliance.github.io/ohcleandat/">{ohcleandat}</a> package. The underlying philosophy of the ETL process is the original data are only lightly modified (stripping white spaces, column names to snake case, etc) while humans do any cleaning that requires thought via validation logs. Changes made in logs are then applied to the data before they are integrated into various larger workpackages. Those workpackages are then deposited into Zenodo to create versioned single sources of truth with digital object identifiers.</p> <pre><code class="language-mermaid">flowchart LR
    A[Dropbox] --&gt; E(ETL in targets with ohcleandat)
    B[GoogleDrive] --&gt; E
    C[Airtable] --&gt; E
    D[ODK] --&gt; E
    J[Validation Logs] --&gt; E
    E --&gt; J
    E --&gt; F[Cleaned Integrated Data]  
    F --&gt; G(Prep for Archive in targets with deposits)
    G --&gt; H{Zenodo}
</code></pre> <h2 id="an-abbreviated-intro-to-deposits">An abbreviated intro to {deposits}</h2> <p>The first thing you have to know about {deposits} is that it uses the R6 class. R6 is an object oriented framework where each class of object has a set of methods (functions) that can be applied to it. This is really nice because you can access all of the available methods of a <code class="language-plaintext highlighter-rouge">depositClient</code> object by using <code class="language-plaintext highlighter-rouge">cli$SOME_METHOD()</code> - where its less convenient for people who work in Rstudio is when you’re looking for the help page for <code class="language-plaintext highlighter-rouge">SOME_METHOD</code> - instead you have to look for <code class="language-plaintext highlighter-rouge">depositClient</code> and scroll to the link to <code class="language-plaintext highlighter-rouge">SOME_METHOD</code>.</p> <p>The next kind of tricky thing is <a href="https://docs.ropensci.org/deposits/articles/install-setup.html#setup-api-tokens">adding api tokens to the environment</a>. This can be done in a number of different ways (<a href="https://ecohealthalliance.github.io/eha-ma-handbook/16-encryption.html">encrypted .env file</a>, <a href="https://usethis.r-lib.org/reference/edit.html">usethis::edit_r_environ</a>, etc) but is essential to using this package. Remember that these tokens are sensitive credentials and should not be openly shared.</p> <p>{deposits} works as an intermediary between a remote service (<a href="https://zenodo.org/">Zenodo</a> or <a href="https://figshare.com/">Figshare</a>) and your local machine. Via {deposits} you can create, read, update, or delete items on a remote service.</p> <pre><code class="language-mermaid">flowchart LR
    A[Local storage] &lt;--&gt; B{depositsClient}
    B &lt;--&gt; C[Zenodo/Figshare]
</code></pre> <p>{deposits} allows you to pre-populate the metadata for those items. This is incredibly useful if you have to deposit many items with similar metadata, if you have highly collaborative items with dozens of co-authors/contributors, or if you want to update many items with the same bit of metadata. You might be asking yourself - do Zenodo and Figshare really use the same terms with the same properties in their APIs? The answer is no, but they both use flavors of <a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#section-1">Dublin Core</a> that Mark has mapped to common standard. {deposits} uses JSON validation to enforce the standard and the package also provides a template properly formatted metadata.</p> <p>Finally, {deposits} allows you to push items to a service and publish them. On zenodo, you may push items up as embargoed or open (restricted is coming soon pending a pull request).</p> <h2 id="what-does-our-workflow-look-like">What does our workflow look like?</h2> <p>We created a <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> that contains all of the DCMI terms we would like to pre-populate across ~35 zenodo deposits. Because certain DCMI terms like creator can have multiple attributes, we augmented the <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> with a <code class="language-plaintext highlighter-rouge">creator_metadata.csv</code> file that contains additional terms. I opted to use csv files because 1) they are very easy to maintain, 2) the are extremely interoperable, 3) we have a very limited number of tables. If there were additional tables to link or more complicated relationships I would use a relational database like MySQL (or Airtable).</p> <pre><code class="language-mermaid">flowchart LR

    A[Deposit Metadata CSV] --&gt; G(Prep for Archive in targets with deposits)
    B[Creator Metadata CSV] --&gt; G
    C[ Data Files]  --&gt; G
    G --&gt; H{Zenodo}
</code></pre> <p><code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code></p> <table> <thead> <tr> <th>title</th> <th>format</th> <th>created</th> <th>creator</th> <th>files</th> <th>…</th> </tr> </thead> <tbody> <tr> <td>Work package 1</td> <td>dataset</td> <td>2024-09-01</td> <td>“Collin, Mindy, Johana”</td> <td>“dat1.csv, dat2.csv”</td> <td>…</td> </tr> </tbody> </table> <p>Each row in <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> corresponds to a work package - a collection of data files that will become a deposit. The PIs on the project requested access to raw, semi-clean, and clean versions of the data. They also provided lists of who should be credited for what and who should have access to what. Unfortunately in Zenodo you cannot restrict access to specific files like you can in OSF, so a different deposit has to be made for each group that needs access.</p> <p>We then map over each row in the dataset in {targets} using <a href="https://docs.ropensci.org/targets/reference/tar_group.html">the group iterator</a> to create the draft deposits that we need. The <code class="language-plaintext highlighter-rouge">depositClient</code> plays very nicely with targets, and can even be tar_loaded when you need to interactively debug.</p> <p>Another important component of our workflow is generating codebooks/data dictionaries/structural metadata for the different work packages. By default, {deposits} creates a {frictionless} data package that describes all the files/resources in a deposit. The <code class="language-plaintext highlighter-rouge">datapackage.json</code> file contains a minimal description of the different files that contains the field name and field type. We can augment this with field descriptions, term URIs, or any number of additional attributes that would helpful when describing the data.<br/> To do this, we use <code class="language-plaintext highlighter-rouge">ohcleandat::create_structural_metadata</code> and <code class="language-plaintext highlighter-rouge">ohcleandat::update_structural_metadata</code> to create/update the codebooks then <code class="language-plaintext highlighter-rouge">ohcleandat::expand_frictionless_metadata()</code> to add those elements to <code class="language-plaintext highlighter-rouge">datapackage.json</code>. Because each deposit contains multiple data files and multiple codebook files, we have to iteratively <code class="language-plaintext highlighter-rouge">purrr::walk</code> over them to properly update the data.</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' purrr::walk over ohcleandat::expand_frictionless_metadata</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @param file_paths List with the elements data and codebook</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @return Invisible. writes updated datapackage.json</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @examples</span><span class="w">
</span><span class="n">walk_expand_frictionless_metadata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  </span><span class="n">purrr</span><span class="o">::</span><span class="n">walk2</span><span class="p">(</span><span class="n">file_paths</span><span class="o">$</span><span class="n">codebook</span><span class="p">,</span><span class="n">file_paths</span><span class="o">$</span><span class="n">data</span><span class="p">,</span><span class="k">function</span><span class="p">(</span><span class="n">codebook_path</span><span class="p">,</span><span class="n">data_path</span><span class="p">){</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="nf">is.na</span><span class="p">(</span><span class="n">codebook_path</span><span class="p">)){</span><span class="w">
      </span><span class="n">msg</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"No codebook listed for %s"</span><span class="p">,</span><span class="n">data_path</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="kc">NA</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
    
    </span><span class="n">codebook_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readr</span><span class="o">::</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codebook_path</span><span class="p">)</span><span class="w">
    </span><span class="c1">## check dataset_name matches resource_name</span><span class="w">
    </span><span class="n">resource_name</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">get_resource_name</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span><span class="w">
    
    </span><span class="k">if</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">codebook_df</span><span class="o">$</span><span class="n">dataset_name</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">resource_name</span><span class="p">){</span><span class="w">
      </span><span class="n">msg</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"resource_name and dataset_name do not match. 

Check that the proper name was added to the codebook and that
the files are in the proper order in deposits_metadata.csv. 
resource_name is taken from the file name for a given dataset.

%s != %s"</span><span class="p">,</span><span class="w">
                    </span><span class="n">resource_name</span><span class="p">,</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">codebook_df</span><span class="o">$</span><span class="n">dataset_name</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w">
      
      </span><span class="n">stop</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
    
    </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"%s/datapackage.json"</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
    
    
    </span><span class="n">ohcleandat</span><span class="o">::</span><span class="n">expand_frictionless_metadata</span><span class="p">(</span><span class="n">structural_metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codebook_df</span><span class="p">,</span><span class="w">
                                             </span><span class="n">resource_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resource_name</span><span class="p">,</span><span class="w"> </span><span class="c1"># name of the file with no extension</span><span class="w">
                                             </span><span class="n">resource_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_path</span><span class="p">,</span><span class="w">
                                             </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_package_path</span><span class="p">)</span><span class="w">
  </span><span class="p">})</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>When we go to add this updated <code class="language-plaintext highlighter-rouge">datapackage.json</code> file to the deposit, the order of operations matters. If we are creating an item, the function might look like this:</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' Create Zenodo Deposit and Expand Metadata</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @param cli deposits client</span><span class="w">
</span><span class="cd">#' @param metadata_dcmi_complete list of DCMI metadata terms</span><span class="w">
</span><span class="cd">#' @param file_paths paths to data files</span><span class="w">
</span><span class="cd">#' @param dir_path path to directory where workpackage files are stored locally</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @return updated deposits client</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @examples</span><span class="w">
</span><span class="n">create_zenodo_deposit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w"> </span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  
  
  </span><span class="c1">## populate descriptive metadata for zenodo</span><span class="w">

  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_fill_metadata</span><span class="p">(</span><span class="n">metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata_dcmi_complete</span><span class="p">)</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_new</span><span class="p">(</span><span class="n">prereserve_doi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## add resources - adds everything in dir_path</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_add_resource</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">

  </span><span class="c1">#upload files  </span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_upload_file</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># expand structural metadata</span><span class="w">
  </span><span class="n">walk_expand_frictionless_metadata</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># update structural metadata - has to be done in this order or datapackage.json is re-written on upload</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_update</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="nf">return</span><span class="p">(</span><span class="n">cli</span><span class="p">)</span><span class="w">
  
</span><span class="p">}</span><span class="w">
  
</span></code></pre></div></div> <p>and updating a deposit might look something like:</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">update_zenodo_deposit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="w"> </span><span class="n">metadata_updated</span><span class="p">,</span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w"> </span><span class="n">file_paths</span><span class="p">,</span><span class="w"> </span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  
  
  </span><span class="c1">## retrieve the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"getting deposit from id"</span><span class="p">)</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_retrieve</span><span class="p">(</span><span class="nf">as.integer</span><span class="p">(</span><span class="n">metadata_updated</span><span class="o">$</span><span class="n">deposit_id</span><span class="p">))</span><span class="w">
  
  </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"%s/datapackage.json"</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## update descriptive metadata</span><span class="w">
  </span><span class="c1">## don't need to walk over this because there is only one set of descriptive for the data package</span><span class="w">
  </span><span class="c1">## metadata for the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"updating descriptive metadata"</span><span class="p">)</span><span class="w">
  </span><span class="n">ohcleandat</span><span class="o">::</span><span class="n">update_frictionless_metadata</span><span class="p">(</span><span class="n">descriptive_metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w">
                                           </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_package_path</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_fill_metadata</span><span class="p">(</span><span class="n">metadata_dcmi_complete</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## expand metadata</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"expanding metadata"</span><span class="p">)</span><span class="w">
  </span><span class="n">walk_expand_frictionless_metadata</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## update the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"updating deposit"</span><span class="p">)</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_upload_file</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"returning cli"</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">cli</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>We can even keep our <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> file updated using the <code class="language-plaintext highlighter-rouge">depositClient</code>.</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' Update deposits metadata</span><span class="w">
</span><span class="cd">#' </span><span class="w">
</span><span class="cd">#' Find workpackage titles that match deposit titles </span><span class="w">
</span><span class="cd">#' </span><span class="w">
</span><span class="cd">#' @param cli deposits Client</span><span class="w">
</span><span class="cd">#' @param metadata_formatted dataframe of formatted descriptive metadata</span><span class="w">
</span><span class="cd">#' @return</span><span class="w">
</span><span class="cd">#' @author Collin Schwantes</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="n">update_zenodo_metadata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="w"> </span><span class="n">metadata_formatted</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="n">df_doi_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">purrr</span><span class="o">::</span><span class="n">map_df</span><span class="p">(</span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">title</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">){</span><span class="w">
        
      </span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">identifier</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA_character_</span><span class="p">,</span><span class="w"> </span><span class="n">deposit_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA_character_</span><span class="p">)</span><span class="w">
    
      </span><span class="n">deposits_found</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cli</span><span class="o">$</span><span class="n">deposits</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
      </span><span class="n">dplyr</span><span class="o">::</span><span class="n">filter</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> 
      </span><span class="n">dplyr</span><span class="o">::</span><span class="n">select</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="p">,</span><span class="w"> </span><span class="n">doi</span><span class="p">)</span><span class="w">
    
    </span><span class="k">if</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">deposits_found</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">){</span><span class="w">
      </span><span class="n">message</span><span class="p">(</span><span class="s2">"No matching items found"</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
      
    </span><span class="k">if</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">deposits_found</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">1</span><span class="p">){</span><span class="w">
      </span><span class="n">warning</span><span class="p">(</span><span class="s2">"clean up your deposits?"</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
      
      </span><span class="n">df</span><span class="o">$</span><span class="n">identifier</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">deposits_found</span><span class="o">$</span><span class="n">doi</span><span class="p">)</span><span class="w">
      </span><span class="n">df</span><span class="o">$</span><span class="n">deposit_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">deposits_found</span><span class="o">$</span><span class="n">id</span><span class="p">)</span><span class="w">
    
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
  </span><span class="p">})</span><span class="w">
  
  
  </span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">identifier</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_doi_id</span><span class="o">$</span><span class="n">identifier</span><span class="w">
  </span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">deposit_id</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_doi_id</span><span class="o">$</span><span class="n">deposit_id</span><span class="w">

  </span><span class="nf">return</span><span class="p">(</span><span class="n">metadata_formatted</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>We can create many deposits with good descriptive metadata, extend the structural metadata, and keep out deposits_metadata.csv up to date using {deposits}, {ohcleandat}, and a little {dplyr}.</p> <h2 id="potential-pain-points">Potential pain points</h2> <p>1) Some of the more complex DCMI terms require nested lists with very particular structures. This can be hard to reason about if you’re not super familiar with <a href="https://eloquentjavascript.net/04_data.html">JSON</a> or how the <a href="https://arxiv.org/abs/1403.2805">{jsonlite}</a> package converts json to R objects. Mark provides good examples of constructing the <code class="language-plaintext highlighter-rouge">creator</code> objects in the deposits documentation. Even if you are a JSON wizard, the <a href="https://developers.zenodo.org/#entities">entities documentation</a> in the Zenodo API is super helpful.<br/> 2) Metadata errors can feel a little cryptic until you get a better understanding of <a href="https://cran.rstudio.com/web/packages/jsonvalidate/vignettes/jsonvalidate.html">JSON validation</a> and stare at the <a href="https://github.com/ropenscilabs/deposits/blob/main/inst/extdata/dc/schema.json">{deposits json schema}</a> for a minute or two. 3) Collaboration can be challenging because drafts have to be manually shared in Zenodo. <code class="language-plaintext highlighter-rouge">¯\_(ツ)_/¯</code>.</p> <h2 id="conclusions">Conclusions</h2> <p>We were able put the deposits package through the wringer with the RVF2 project and it performed extremely well. The {deposits} package is great for making and managing a collection of Zenodo deposits. It takes a second to get the hang of the <code class="language-plaintext highlighter-rouge">R6</code> object oriented structure and JSON data validation, but once you do, the thoughtful package design results in a smooth workflow whether you’re updating a single deposit or a large batch.</p> <pre><code class="language-mermaid">flowchart LR

    A[Deposit Metadata CSV] --&gt; G(Prep for Archive in targets with deposits)
    B[Creator Metadata CSV] --&gt; G
    C[Workpackage Files]  --&gt; G
    G --&gt; H{Zenodo}
    G ---&gt; A
</code></pre>]]></content><author><name></name></author><category term="deposits"/><category term="zenodo"/><category term="FAIR"/><category term="data"/><summary type="html"><![CDATA[A blog post about using the R package deposits]]></summary></entry><entry><title type="html">Zero Shot Data Integration</title><link href="https://collinschwantes.github.io/blog/2024/zero-shot-data-integration/" rel="alternate" type="text/html" title="Zero Shot Data Integration"/><published>2024-08-30T17:39:00+00:00</published><updated>2024-08-30T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2024/zero-shot-data-integration</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2024/zero-shot-data-integration/"><![CDATA[<p>During a recent interview I was asked how I might approach integrating data programatically without ever having seen it. Depending on the parameters, this sounds like a wickedly difficult and fun problem so I plan on exploring it further here.</p> <p>My typical approach to integrating datasets - combining two datasets that contain analogous or identical data elements - involves carefully reviewing metadata and data. This involves comparing data structures, field descriptions, and field properties to ensure the datasets can actually be combined.</p> <p>But what happens if the volume of data is too great to review manually?</p> <p>Ideally, this system would examine incoming data, identify candidates for integration, and attempt to integrate them. Integrated items would then be reviewed for consistency and include a log describing differences between the datasets, and transformations or modifications made to facilitate integration. Data provenance would be key to all these approaches.</p> <h3 id="strict-standards-based-approach">Strict Standards based approach</h3> <p>Anyone submitting data must use a specified data standard and include valid metadata describing their submission.</p> <p>Data is lightly processed to ensure that metadata are valid and data align to the metadata. Datasets describing the same phenomena can then be integrated.</p> <table> <thead> <tr> <th>GUID</th> <th>Local ID</th> <th>Dataset ID</th> <th>Age</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>A294</td> <td>1</td> <td>42</td> </tr> <tr> <td>2</td> <td>foo</td> <td>2</td> <td>42</td> </tr> <tr> <td>3</td> <td>0001</td> <td>3</td> <td>42</td> </tr> </tbody> </table> <p>This method works well if you’re an organization that can make these kinds of requirements (think NIH’s GenBank or GBIF) but is probably too restrictive for post-hoc data harmonization activities or smaller repos.</p> <h3 id="generalized-standards-approach">Generalized standards approach</h3> <p>This approach imposes rules on data upload that require adherence to a general standard like <a href="https://specs.frictionlessdata.io/data-package/">frictionless tabular data package</a>. Frictionless provides a minimal framework for metadata and eliminates the need to guess about file structure, file types, and metadata file names. Within the frictionless standard, someone can extend the structural and descriptive metadata as they see fit but the structure of the data package will be unchanged.</p> <p>In this scenario I can focus on leveraging the structural and descriptive metadata to look for similarities between two datasets - identifying candidates for matching. This could involve parsing descriptive metadata looking for matching terms and comparing their values.</p> <p>That comparison could involve creating vectorized representations of dataset descriptions and looking for close or exact matches that would suggest where further more computationally expensive comparisons should happen.</p> <p>The structural metadata would also be extremely useful. Datasets with field names that (mostly) match could be candidates for integration, especially if matching field names are reinforced by (mostly) matching field descriptions or better yet exactly matching term IRIs.</p> <pre><code class="language-mermaid">flowchart LR
    A[Data Upload] --&gt; D(Descriptive Metadata)
    A --&gt; S(Structural Metadata) 
    A --&gt; C(CSV)
    D --&gt; P[Identify Candidates]
    S --&gt; P[Identify Candidates]
    C --&gt; P[Identify Candidates]
    P --&gt; I[Integrate Candidates]
    I --&gt; L[Integration Log]
    I --&gt; F[Integrated Dataset]
    I --&gt; M[Integrated Metadata]

</code></pre> <p>Integrated datasets would receive a confidence score 0-1, 1 being extremely confident in the integration. A person would need to confirm that the datasets can be integrated. We could then setup reinforcement learning to improve model performance.</p> <h2 id="matching-on-name-and-distributionsvalues">Matching on name and distributions/values</h2> <p>It might also be possible to integrate data blindly by using a combination of variable name and the distributions of values in the data. Confidence in matches could be boosted by metadata but essentially if two datasets contain columns with similar names (weight vs mass) and the values have similar distributions (e.g. normally distributed data with ranges in a certain tolerance). Ideally a single model would be able to handle many different data types and correctly classify multiple data types (dates, coordinates, strings, etc). This would require a good initial parsing algorithm and a lot of training data.</p> <h2 id="relax-the-structure">Relax the structure</h2> <p>What if certain elements of data can be consistently harmonized across datasets, but the entire datasets themselves can’t be integrated?</p> <p>Data submitted in tabular, tree, or other structures could be unspooled and integrated at the individual property level using one of the matching algorithms described above. If we capture data relationships in a graph we have something that is pretty exciting. Not only can we create a large dataset for a particular attribute, but we can then query the graph of relationships and figure out what other attributes can be added to the data.</p> <pre><code class="language-mermaid">flowchart LR

    %% dataset 1
    f[foo] --&gt;|Data_1| b(bar)
    f--&gt; D(Data_1)
    b --&gt; D

    %% dataset 2
    l[lorum] --&gt; |Data_2| f
    l --&gt; |Data_2| b
    i[ipsum] --&gt; |Data_2| l
    i --&gt; |Data_2| f
    i --&gt; |Data_2| b
    f --&gt; |Data_2| b

    l --&gt; D2(Data_2)
    f --&gt; D2
    i --&gt; D2
    b --&gt; D2
    
</code></pre>]]></content><author><name></name></author><category term="data"/><category term="integration"/><category term="processing"/><category term="FAIR"/><category term="harmonization"/><summary type="html"><![CDATA[A blog post about integrating data sight unseen]]></summary></entry></feed>