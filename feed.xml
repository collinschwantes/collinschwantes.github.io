<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://collinschwantes.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://collinschwantes.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-10T17:15:21+00:00</updated><id>https://collinschwantes.github.io/feed.xml</id><title type="html">blank</title><subtitle>a personal website for Collin Schwantes </subtitle><entry><title type="html">Deposits in the Wild</title><link href="https://collinschwantes.github.io/blog/2024/deposits-in-the-wild/" rel="alternate" type="text/html" title="Deposits in the Wild"/><published>2024-08-30T17:39:00+00:00</published><updated>2024-08-30T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2024/deposits-in-the-wild</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2024/deposits-in-the-wild/"><![CDATA[<p>This blog post has been published on the <a href="https://ropensci.org/blog/2024/09/17/deposits-in-the-wild/">ROpenSci blog</a></p> <p>For the better part of a year, I have been looking for an opportunity to use the ROpenSci package <a href="https://docs.ropensci.org/deposits/">{deposits}</a> in my role as the Data Librarian at EcoHealth Alliance. I had done some initial testing with the Mark Padgham, the brilliant person who developed this package, but there weren’t any projects ready for me to put {deposits} through its paces. Enter the <em>Rift Valley Fever Virus in South Africa</em> project, a ten year, multiple part study of humans, wildlife (mosquitoes and wild ungulates), and domestic animals that uses every data store from dropbox to Google drive to Airtable to ODK with a healthy mix file formats for tabular data. Additionally, the PIs on the project are very enthusiastic about making the data FAIR.</p> <p>The team and I put together workflow in <a href="https://books.ropensci.org/targets/">{targets}</a> with the mechanics of ETL largely handled by our <a href="https://ecohealthalliance.github.io/ohcleandat/">{ohcleandat}</a> package. The underlying philosophy of the ETL process is the original data are only lightly modified (stripping white spaces, column names to snake case, etc) while humans do any cleaning that requires thought via validation logs. Changes made in logs are then applied to the data before they are integrated into various larger workpackages. Those workpackages are then deposited into Zenodo to create versioned single sources of truth with digital object identifiers.</p> <pre><code class="language-mermaid">flowchart LR
    A[Dropbox] --&gt; E(ETL in targets with ohcleandat)
    B[GoogleDrive] --&gt; E
    C[Airtable] --&gt; E
    D[ODK] --&gt; E
    J[Validation Logs] --&gt; E
    E --&gt; J
    E --&gt; F[Cleaned Integrated Data]  
    F --&gt; G(Prep for Archive in targets with deposits)
    G --&gt; H{Zenodo}
</code></pre> <h2 id="an-abbreviated-intro-to-deposits">An abbreviated intro to {deposits}</h2> <p>The first thing you have to know about {deposits} is that it uses the R6 class. R6 is an object oriented framework where each class of object has a set of methods (functions) that can be applied to it. This is really nice because you can access all of the available methods of a <code class="language-plaintext highlighter-rouge">depositClient</code> object by using <code class="language-plaintext highlighter-rouge">cli$SOME_METHOD()</code> - where its less convenient for people who work in Rstudio is when you’re looking for the help page for <code class="language-plaintext highlighter-rouge">SOME_METHOD</code> - instead you have to look for <code class="language-plaintext highlighter-rouge">depositClient</code> and scroll to the link to <code class="language-plaintext highlighter-rouge">SOME_METHOD</code>.</p> <p>The next kind of tricky thing is <a href="https://docs.ropensci.org/deposits/articles/install-setup.html#setup-api-tokens">adding api tokens to the environment</a>. This can be done in a number of different ways (<a href="https://ecohealthalliance.github.io/eha-ma-handbook/16-encryption.html">encrypted .env file</a>, <a href="https://usethis.r-lib.org/reference/edit.html">usethis::edit_r_environ</a>, etc) but is essential to using this package. Remember that these tokens are sensitive credentials and should not be openly shared.</p> <p>{deposits} works as an intermediary between a remote service (<a href="https://zenodo.org/">Zenodo</a> or <a href="https://figshare.com/">Figshare</a>) and your local machine. Via {deposits} you can create, read, update, or delete items on a remote service.</p> <pre><code class="language-mermaid">flowchart LR
    A[Local storage] &lt;--&gt; B{depositsClient}
    B &lt;--&gt; C[Zenodo/Figshare]
</code></pre> <p>{deposits} allows you to pre-populate the metadata for those items. This is incredibly useful if you have to deposit many items with similar metadata, if you have highly collaborative items with dozens of co-authors/contributors, or if you want to update many items with the same bit of metadata. You might be asking yourself - do Zenodo and Figshare really use the same terms with the same properties in their APIs? The answer is no, but they both use flavors of <a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#section-1">Dublin Core</a> that Mark has mapped to common standard. {deposits} uses JSON validation to enforce the standard and the package also provides a template properly formatted metadata.</p> <p>Finally, {deposits} allows you to push items to a service and publish them. On zenodo, you may push items up as embargoed or open (restricted is coming soon pending a pull request).</p> <h2 id="what-does-our-workflow-look-like">What does our workflow look like?</h2> <p>We created a <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> that contains all of the DCMI terms we would like to pre-populate across ~35 zenodo deposits. Because certain DCMI terms like creator can have multiple attributes, we augmented the <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> with a <code class="language-plaintext highlighter-rouge">creator_metadata.csv</code> file that contains additional terms. I opted to use csv files because 1) they are very easy to maintain, 2) the are extremely interoperable, 3) we have a very limited number of tables. If there were additional tables to link or more complicated relationships I would use a relational database like MySQL (or Airtable).</p> <pre><code class="language-mermaid">flowchart LR

    A[Deposit Metadata CSV] --&gt; G(Prep for Archive in targets with deposits)
    B[Creator Metadata CSV] --&gt; G
    C[ Data Files]  --&gt; G
    G --&gt; H{Zenodo}
</code></pre> <p><code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code></p> <table> <thead> <tr> <th>title</th> <th>format</th> <th>created</th> <th>creator</th> <th>files</th> <th>…</th> </tr> </thead> <tbody> <tr> <td>Work package 1</td> <td>dataset</td> <td>2024-09-01</td> <td>“Collin, Mindy, Johana”</td> <td>“dat1.csv, dat2.csv”</td> <td>…</td> </tr> </tbody> </table> <p>Each row in <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> corresponds to a work package - a collection of data files that will become a deposit. The PIs on the project requested access to raw, semi-clean, and clean versions of the data. They also provided lists of who should be credited for what and who should have access to what. Unfortunately in Zenodo you cannot restrict access to specific files like you can in OSF, so a different deposit has to be made for each group that needs access.</p> <p>We then map over each row in the dataset in {targets} using <a href="https://docs.ropensci.org/targets/reference/tar_group.html">the group iterator</a> to create the draft deposits that we need. The <code class="language-plaintext highlighter-rouge">depositClient</code> plays very nicely with targets, and can even be tar_loaded when you need to interactively debug.</p> <p>Another important component of our workflow is generating codebooks/data dictionaries/structural metadata for the different work packages. By default, {deposits} creates a {frictionless} data package that describes all the files/resources in a deposit. The <code class="language-plaintext highlighter-rouge">datapackage.json</code> file contains a minimal description of the different files that contains the field name and field type. We can augment this with field descriptions, term URIs, or any number of additional attributes that would helpful when describing the data.<br/> To do this, we use <code class="language-plaintext highlighter-rouge">ohcleandat::create_structural_metadata</code> and <code class="language-plaintext highlighter-rouge">ohcleandat::update_structural_metadata</code> to create/update the codebooks then <code class="language-plaintext highlighter-rouge">ohcleandat::expand_frictionless_metadata()</code> to add those elements to <code class="language-plaintext highlighter-rouge">datapackage.json</code>. Because each deposit contains multiple data files and multiple codebook files, we have to iteratively <code class="language-plaintext highlighter-rouge">purrr::walk</code> over them to properly update the data.</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' purrr::walk over ohcleandat::expand_frictionless_metadata</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @param file_paths List with the elements data and codebook</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @return Invisible. writes updated datapackage.json</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @examples</span><span class="w">
</span><span class="n">walk_expand_frictionless_metadata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  </span><span class="n">purrr</span><span class="o">::</span><span class="n">walk2</span><span class="p">(</span><span class="n">file_paths</span><span class="o">$</span><span class="n">codebook</span><span class="p">,</span><span class="n">file_paths</span><span class="o">$</span><span class="n">data</span><span class="p">,</span><span class="k">function</span><span class="p">(</span><span class="n">codebook_path</span><span class="p">,</span><span class="n">data_path</span><span class="p">){</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="nf">is.na</span><span class="p">(</span><span class="n">codebook_path</span><span class="p">)){</span><span class="w">
      </span><span class="n">msg</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"No codebook listed for %s"</span><span class="p">,</span><span class="n">data_path</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="kc">NA</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
    
    </span><span class="n">codebook_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readr</span><span class="o">::</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codebook_path</span><span class="p">)</span><span class="w">
    </span><span class="c1">## check dataset_name matches resource_name</span><span class="w">
    </span><span class="n">resource_name</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">get_resource_name</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span><span class="w">
    
    </span><span class="k">if</span><span class="p">(</span><span class="n">unique</span><span class="p">(</span><span class="n">codebook_df</span><span class="o">$</span><span class="n">dataset_name</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">resource_name</span><span class="p">){</span><span class="w">
      </span><span class="n">msg</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"resource_name and dataset_name do not match. 

Check that the proper name was added to the codebook and that
the files are in the proper order in deposits_metadata.csv. 
resource_name is taken from the file name for a given dataset.

%s != %s"</span><span class="p">,</span><span class="w">
                    </span><span class="n">resource_name</span><span class="p">,</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">codebook_df</span><span class="o">$</span><span class="n">dataset_name</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w">
      
      </span><span class="n">stop</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
    
    </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"%s/datapackage.json"</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
    
    
    </span><span class="n">ohcleandat</span><span class="o">::</span><span class="n">expand_frictionless_metadata</span><span class="p">(</span><span class="n">structural_metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codebook_df</span><span class="p">,</span><span class="w">
                                             </span><span class="n">resource_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resource_name</span><span class="p">,</span><span class="w"> </span><span class="c1"># name of the file with no extension</span><span class="w">
                                             </span><span class="n">resource_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_path</span><span class="p">,</span><span class="w">
                                             </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_package_path</span><span class="p">)</span><span class="w">
  </span><span class="p">})</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>When we go to add this updated <code class="language-plaintext highlighter-rouge">datapackage.json</code> file to the deposit, the order of operations matters. If we are creating an item, the function might look like this:</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' Create Zenodo Deposit and Expand Metadata</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @param cli deposits client</span><span class="w">
</span><span class="cd">#' @param metadata_dcmi_complete list of DCMI metadata terms</span><span class="w">
</span><span class="cd">#' @param file_paths paths to data files</span><span class="w">
</span><span class="cd">#' @param dir_path path to directory where workpackage files are stored locally</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @return updated deposits client</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="cd">#'</span><span class="w">
</span><span class="cd">#' @examples</span><span class="w">
</span><span class="n">create_zenodo_deposit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w"> </span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  
  
  </span><span class="c1">## populate descriptive metadata for zenodo</span><span class="w">

  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_fill_metadata</span><span class="p">(</span><span class="n">metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata_dcmi_complete</span><span class="p">)</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_new</span><span class="p">(</span><span class="n">prereserve_doi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## add resources - adds everything in dir_path</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_add_resource</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">

  </span><span class="c1">#upload files  </span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_upload_file</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># expand structural metadata</span><span class="w">
  </span><span class="n">walk_expand_frictionless_metadata</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># update structural metadata - has to be done in this order or datapackage.json is re-written on upload</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_update</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="nf">return</span><span class="p">(</span><span class="n">cli</span><span class="p">)</span><span class="w">
  
</span><span class="p">}</span><span class="w">
  
</span></code></pre></div></div> <p>and updating a deposit might look something like:</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">update_zenodo_deposit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="w"> </span><span class="n">metadata_updated</span><span class="p">,</span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w"> </span><span class="n">file_paths</span><span class="p">,</span><span class="w"> </span><span class="n">dir_path</span><span class="p">){</span><span class="w">
  
  
  </span><span class="c1">## retrieve the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"getting deposit from id"</span><span class="p">)</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_retrieve</span><span class="p">(</span><span class="nf">as.integer</span><span class="p">(</span><span class="n">metadata_updated</span><span class="o">$</span><span class="n">deposit_id</span><span class="p">))</span><span class="w">
  
  </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sprintf</span><span class="p">(</span><span class="s2">"%s/datapackage.json"</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## update descriptive metadata</span><span class="w">
  </span><span class="c1">## don't need to walk over this because there is only one set of descriptive for the data package</span><span class="w">
  </span><span class="c1">## metadata for the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"updating descriptive metadata"</span><span class="p">)</span><span class="w">
  </span><span class="n">ohcleandat</span><span class="o">::</span><span class="n">update_frictionless_metadata</span><span class="p">(</span><span class="n">descriptive_metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata_dcmi_complete</span><span class="p">,</span><span class="w">
                                           </span><span class="n">data_package_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_package_path</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_fill_metadata</span><span class="p">(</span><span class="n">metadata_dcmi_complete</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## expand metadata</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"expanding metadata"</span><span class="p">)</span><span class="w">
  </span><span class="n">walk_expand_frictionless_metadata</span><span class="p">(</span><span class="n">file_paths</span><span class="p">,</span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  </span><span class="c1">## update the deposit</span><span class="w">
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"updating deposit"</span><span class="p">)</span><span class="w">
  </span><span class="n">cli</span><span class="o">$</span><span class="n">deposit_upload_file</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dir_path</span><span class="p">)</span><span class="w">
  
  
  </span><span class="n">print</span><span class="p">(</span><span class="s2">"returning cli"</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">cli</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>We can even keep our <code class="language-plaintext highlighter-rouge">deposits_metadata.csv</code> file updated using the <code class="language-plaintext highlighter-rouge">depositClient</code>.</p> <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cd">#' Update deposits metadata</span><span class="w">
</span><span class="cd">#' </span><span class="w">
</span><span class="cd">#' Find workpackage titles that match deposit titles </span><span class="w">
</span><span class="cd">#' </span><span class="w">
</span><span class="cd">#' @param cli deposits Client</span><span class="w">
</span><span class="cd">#' @param metadata_formatted dataframe of formatted descriptive metadata</span><span class="w">
</span><span class="cd">#' @return</span><span class="w">
</span><span class="cd">#' @author Collin Schwantes</span><span class="w">
</span><span class="cd">#' @export</span><span class="w">
</span><span class="n">update_zenodo_metadata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">cli</span><span class="p">,</span><span class="w"> </span><span class="n">metadata_formatted</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="n">df_doi_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">purrr</span><span class="o">::</span><span class="n">map_df</span><span class="p">(</span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">title</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">){</span><span class="w">
        
      </span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">identifier</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA_character_</span><span class="p">,</span><span class="w"> </span><span class="n">deposit_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA_character_</span><span class="p">)</span><span class="w">
    
      </span><span class="n">deposits_found</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cli</span><span class="o">$</span><span class="n">deposits</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
      </span><span class="n">dplyr</span><span class="o">::</span><span class="n">filter</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> 
      </span><span class="n">dplyr</span><span class="o">::</span><span class="n">select</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="p">,</span><span class="w"> </span><span class="n">doi</span><span class="p">)</span><span class="w">
    
    </span><span class="k">if</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">deposits_found</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">){</span><span class="w">
      </span><span class="n">message</span><span class="p">(</span><span class="s2">"No matching items found"</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
      
    </span><span class="k">if</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">deposits_found</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">1</span><span class="p">){</span><span class="w">
      </span><span class="n">warning</span><span class="p">(</span><span class="s2">"clean up your deposits?"</span><span class="p">)</span><span class="w">
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
    </span><span class="p">}</span><span class="w">
      
      </span><span class="n">df</span><span class="o">$</span><span class="n">identifier</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">deposits_found</span><span class="o">$</span><span class="n">doi</span><span class="p">)</span><span class="w">
      </span><span class="n">df</span><span class="o">$</span><span class="n">deposit_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">deposits_found</span><span class="o">$</span><span class="n">id</span><span class="p">)</span><span class="w">
    
      </span><span class="nf">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
  </span><span class="p">})</span><span class="w">
  
  
  </span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">identifier</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_doi_id</span><span class="o">$</span><span class="n">identifier</span><span class="w">
  </span><span class="n">metadata_formatted</span><span class="o">$</span><span class="n">deposit_id</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_doi_id</span><span class="o">$</span><span class="n">deposit_id</span><span class="w">

  </span><span class="nf">return</span><span class="p">(</span><span class="n">metadata_formatted</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>We can create many deposits with good descriptive metadata, extend the structural metadata, and keep out deposits_metadata.csv up to date using {deposits}, {ohcleandat}, and a little {dplyr}.</p> <h2 id="potential-pain-points">Potential pain points</h2> <p>1) Some of the more complex DCMI terms require nested lists with very particular structures. This can be hard to reason about if you’re not super familiar with <a href="https://eloquentjavascript.net/04_data.html">JSON</a> or how the <a href="https://arxiv.org/abs/1403.2805">{jsonlite}</a> package converts json to R objects. Mark provides good examples of constructing the <code class="language-plaintext highlighter-rouge">creator</code> objects in the deposits documentation. Even if you are a JSON wizard, the <a href="https://developers.zenodo.org/#entities">entities documentation</a> in the Zenodo API is super helpful.<br/> 2) Metadata errors can feel a little cryptic until you get a better understanding of <a href="https://cran.rstudio.com/web/packages/jsonvalidate/vignettes/jsonvalidate.html">JSON validation</a> and stare at the <a href="https://github.com/ropenscilabs/deposits/blob/main/inst/extdata/dc/schema.json">{deposits json schema}</a> for a minute or two. 3) Collaboration can be challenging because drafts have to be manually shared in Zenodo. <code class="language-plaintext highlighter-rouge">¯\_(ツ)_/¯</code>.</p> <h2 id="conclusions">Conclusions</h2> <p>We were able put the deposits package through the wringer with the RVF2 project and it performed extremely well. The {deposits} package is great for making and managing a collection of Zenodo deposits. It takes a second to get the hang of the <code class="language-plaintext highlighter-rouge">R6</code> object oriented structure and JSON data validation, but once you do, the thoughtful package design results in a smooth workflow whether you’re updating a single deposit or a large batch.</p> <pre><code class="language-mermaid">flowchart LR

    A[Deposit Metadata CSV] --&gt; G(Prep for Archive in targets with deposits)
    B[Creator Metadata CSV] --&gt; G
    C[Workpackage Files]  --&gt; G
    G --&gt; H{Zenodo}
    G ---&gt; A
</code></pre>]]></content><author><name></name></author><category term="deposits"/><category term="zenodo"/><category term="FAIR"/><category term="data"/><summary type="html"><![CDATA[A blog post about using the R package deposits]]></summary></entry><entry><title type="html">Zero Shot Data Integration</title><link href="https://collinschwantes.github.io/blog/2024/zero-shot-data-integration/" rel="alternate" type="text/html" title="Zero Shot Data Integration"/><published>2024-08-30T17:39:00+00:00</published><updated>2024-08-30T17:39:00+00:00</updated><id>https://collinschwantes.github.io/blog/2024/zero-shot-data-integration</id><content type="html" xml:base="https://collinschwantes.github.io/blog/2024/zero-shot-data-integration/"><![CDATA[<p>During a recent interview I was asked how I might approach integrating data programatically without ever having seen it. Depending on the parameters, this sounds like a wickedly difficult and fun problem so I plan on exploring it further here.</p> <p>My typical approach to integrating datasets - combining two datasets that contain analogous or identical data elements - involves carefully reviewing metadata and data. This involves comparing data structures, field descriptions, and field properties to ensure the datasets can actually be combined.</p> <p>But what happens if the volume of data is too great to review manually?</p> <p>Ideally, this system would examine incoming data, identify candidates for integration, and attempt to integrate them. Integrated items would then be reviewed for consistency and include a log describing differences between the datasets, and transformations or modifications made to facilitate integration. Data provenance would be key to all these approaches.</p> <h3 id="strict-standards-based-approach">Strict Standards based approach</h3> <p>Anyone submitting data must use a specified data standard and include valid metadata describing their submission.</p> <p>Data is lightly processed to ensure that metadata are valid and data align to the metadata. Datasets describing the same phenomena can then be integrated.</p> <table> <thead> <tr> <th>GUID</th> <th>Local ID</th> <th>Dataset ID</th> <th>Age</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>A294</td> <td>1</td> <td>42</td> </tr> <tr> <td>2</td> <td>foo</td> <td>2</td> <td>42</td> </tr> <tr> <td>3</td> <td>0001</td> <td>3</td> <td>42</td> </tr> </tbody> </table> <p>This method works well if you’re an organization that can make these kinds of requirements (think NIH’s GenBank or GBIF) but is probably too restrictive for post-hoc data harmonization activities or smaller repos.</p> <h3 id="generalized-standards-approach">Generalized standards approach</h3> <p>This approach imposes rules on data upload that require adherence to a general standard like <a href="https://specs.frictionlessdata.io/data-package/">frictionless tabular data package</a>. Frictionless provides a minimal framework for metadata and eliminates the need to guess about file structure, file types, and metadata file names. Within the frictionless standard, someone can extend the structural and descriptive metadata as they see fit but the structure of the data package will be unchanged.</p> <p>In this scenario I can focus on leveraging the structural and descriptive metadata to look for similarities between two datasets - identifying candidates for matching. This could involve parsing descriptive metadata looking for matching terms and comparing their values.</p> <p>That comparison could involve creating vectorized representations of dataset descriptions and looking for close or exact matches that would suggest where further more computationally expensive comparisons should happen.</p> <p>The structural metadata would also be extremely useful. Datasets with field names that (mostly) match could be candidates for integration, especially if matching field names are reinforced by (mostly) matching field descriptions or better yet exactly matching term IRIs.</p> <pre><code class="language-mermaid">flowchart LR
    A[Data Upload] --&gt; D(Descriptive Metadata)
    A --&gt; S(Structural Metadata) 
    A --&gt; C(CSV)
    D --&gt; P[Identify Candidates]
    S --&gt; P[Identify Candidates]
    C --&gt; P[Identify Candidates]
    P --&gt; I[Integrate Candidates]
    I --&gt; L[Integration Log]
    I --&gt; F[Integrated Dataset]
    I --&gt; M[Integrated Metadata]

</code></pre> <p>Integrated datasets would receive a confidence score 0-1, 1 being extremely confident in the integration. A person would need to confirm that the datasets can be integrated. We could then setup reinforcement learning to improve model performance.</p> <h2 id="matching-on-name-and-distributionsvalues">Matching on name and distributions/values</h2> <p>It might also be possible to integrate data blindly by using a combination of variable name and the distributions of values in the data. Confidence in matches could be boosted by metadata but essentially if two datasets contain columns with similar names (weight vs mass) and the values have similar distributions (e.g. normally distributed data with ranges in a certain tolerance). Ideally a single model would be able to handle many different data types and correctly classify multiple data types (dates, coordinates, strings, etc). This would require a good initial parsing algorithm and a lot of training data.</p> <h2 id="relax-the-structure">Relax the structure</h2> <p>What if certain elements of data can be consistently harmonized across datasets, but the entire datasets themselves can’t be integrated?</p> <p>Data submitted in tabular, tree, or other structures could be unspooled and integrated at the individual property level using one of the matching algorithms described above. If we capture data relationships in a graph we have something that is pretty exciting. Not only can we create a large dataset for a particular attribute, but we can then query the graph of relationships and figure out what other attributes can be added to the data.</p> <pre><code class="language-mermaid">flowchart LR

    %% dataset 1
    f[foo] --&gt;|Data_1| b(bar)
    f--&gt; D(Data_1)
    b --&gt; D

    %% dataset 2
    l[lorum] --&gt; |Data_2| f
    l --&gt; |Data_2| b
    i[ipsum] --&gt; |Data_2| l
    i --&gt; |Data_2| f
    i --&gt; |Data_2| b
    f --&gt; |Data_2| b

    l --&gt; D2(Data_2)
    f --&gt; D2
    i --&gt; D2
    b --&gt; D2
    
</code></pre>]]></content><author><name></name></author><category term="data"/><category term="integration"/><category term="processing"/><category term="FAIR"/><category term="harmonization"/><summary type="html"><![CDATA[A blog post about integrating data sight unseen]]></summary></entry></feed>